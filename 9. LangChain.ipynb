{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ea418-0d2c-4563-96fd-9f2e174b3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LangChain\n",
    "#https://python.langchain.com/docs/get_started/introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917500c0-9f38-4a10-9149-fbc4229d216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6780e04-8a4b-4243-b0ce-4e63655ee05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '<your key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47725b53-cbe8-4874-a7ae-e2699bf389e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a197383b-4d91-4541-a98f-a3be2a2c9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfae44d-26e4-4616-8ba9-466dd5834bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd6dbd5-40fe-4606-bac9-720afc966333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When you mention a big iB... ¬ª read more\n",
      "\n",
      "MoU with Nissan Technology Center A Memorandum of Understanding (MoU) was signed by Mattias Astrom, the executive vice president of Research and Development Center at Toyota Motor Europe, and Carlos Tavares, CEO of Nissan Motors, etc. at the Nissan Motor Company‚Äôs headquarters in Yokohama, Japan on March 8th. This MoU would pave the path to partnership between Toyota and Nissan to maximize t... ¬ª read more\n",
      "\n",
      "With the tremendous growth of data and demand for 5G and wireless services, RF technology is evolving as a strategic industry enabler (up to 100 GHz and perhaps beyond). At similar pricing levels, transistor technology is getting very mature in wireless applications, and advanced CMOS scaling options may be able to provide 40-50% gains for a given application. But RF devices are evolved via ... ¬ª read more\n",
      "\n",
      "Synopsys' Brian Fuller and Anne-Francoise Pellet chat about RISC-V cores, security, and what to consider when considering a RISC-V core. Mentor's Colin Walls examines safety considerations for data output buffers in safety-critical applications. Synopsys' Pravin M\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name='davinci-002', temperature=1, max_tokens=245)\n",
    "response = llm('Write a tweet about large-language models.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "654820a6-48d3-4fed-a0c8-b9028435bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' Chicago\".\\n\\nQ: Spoken Language\\nA: a) The first language you learn to speak\\nb)may develop a dialect:\\na) totally characteristic to a local group (regionalisms)\\nb) shows a diglossia\\nc)–¥–æ—Å–ª—ñ–≤–Ω–æ: \"two different forms of speech; (–Ü–Ω—Ç—Ä. –Ü–Ω—Ç–ª–æ–≥–≥.); Two completely different registers of the same language expected for different situations\"\\n\\nQ: Secondary language\\nA: a second (or more) language which you learn to speak or write.\\nspoken b) as a part of a person\\'s indigenous speech or c) in the modern time/ in a modern situation)\\nc) A language that someone becomes knowledgeable about through cultural exposure, translations allow a speaker to use secondary languages to express themselves.\\n\\nQ: Mother Tongue\\nA: Will be first; also called native tongue home galaxies develops when\\na child is learning to speak.\\n\\nQ: Standard Language\\nA: a) can develop without government help, with government/agencies support, a language\\nshare by all or most speakers of a region who attempt to control community speech.\\nb) unique written and phonetically standard speech recognizes', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text=\" 41% VS. Blues are my favorite, especially light blue and sky blue. Think about it this ...\\n\\nWho's your favorite artist? 33% VS. Music ‚Äì Andrew Lative. People often often m... ...\\n\\nDo you like reading books? 54% VS. I‚Äôve never been a huge book reader unti...\\n\\nWhich drink do you prefer? 41% VS. I love soft drinks and not only - Jell... Ads\\n\\n\", generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 351, 'completion_tokens': 336, 'prompt_tokens': 15}, 'model_name': 'davinci-002'} run=[RunInfo(run_id=UUID('51d0a8c4-3799-4bda-93ba-309599442c7a')), RunInfo(run_id=UUID('6adfda74-36de-4877-b433-c3249625e30a'))]\n"
     ]
    }
   ],
   "source": [
    "responses = llm.generate(['what is the capital of the U.S.?', 'What is the best color?'])\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678067f0-95a3-47ae-9438-8b4b24168e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58d326ed-9d29-482f-bfff-3990cafdf8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Excited to see the incredible potential unlocked by generative AI technology! From music and art to storytelling, the possibilities are endless. ü§ñ‚ú® #GenerativeAI #Innovation #TechRevolution' response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 24, 'total_tokens': 66}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None} id='run-5cf3b276-a3ac-4a33-9262-7920a1ef1691-0'\n"
     ]
    }
   ],
   "source": [
    "response = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1, max_tokens=100)\n",
    "\n",
    "messages = [\n",
    "  SystemMessage(content='you are a social media expert'),\n",
    "  HumanMessage(content='write a tweet about generative ai')\n",
    "]\n",
    "\n",
    "print(response(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34f6109f-b6ef-4259-aca8-1bdf1186daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a367a23e-c905-4b03-ae25-fe4ec210e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will not be on a test or quiz\n",
      "Illustrate each paragraph\n",
      "Click Link Below\n",
      "DiscussRome\n",
      "West Side Story\n",
      "Did you know that West Side Story is loosely based on the William Shakespeare play Romeo and Juliet? Romeo and Juliet is about two families fighting against each other. Families can make so much easier if only they get along. We need to put an end to the fighting between the Republican and Democratic Parties. We can put an end to the fighting over political issues in the city. We can work together, mixing together, even thoigh we are complete opposites.\n",
      "Change and Reconciliation in Britain\n",
      "Made by Duke Ellington Middle School-city council team\n",
      "Introduction\n",
      "1. Summary of year 10 lessons ‚Ä¢ Lesson 24: Great Britain on the eve of the Second World War: change and crisis ‚Ä¢ Lesson 25: Life under a total war economy ‚Ä¢ Lesson 26: The war in the air: the Battle of Britain ‚Ä¢ Lesson 27: Diplomacy, conscription and duvet bombings ‚Ä¢ Lesson 28: The London bombings ‚Ä¢ Lesson 29: The Blitz and the Battle of Britain panorama ‚Ä¢ Lesson 30: Dunkirk ‚Ä¢ Lesson 31: Britain on the brink and the\n"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "You are a expert historian\n",
    "Write a paragraph about {event} in {year}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    " input_variables=['event', 'year'],\n",
    " template=template\n",
    ")\n",
    "\n",
    "response = llm(prompt.format(event='war', year='1863'))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91cee6a4-e7ad-4a3f-9ece-711311fd83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "# from langchain_openai import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7e231f5-63ba-45e9-af98-1efa6a329d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ann\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain about appendix a\n",
      "Go to appendix a and copy the paragraph in solution 2.\n",
      "In groups\n",
      "Work with a group of 4 or 5.\n",
      "You should make a list of facts about war in 1863.\n",
      "you need to be able to write about 4 facts that are not in appendix a.\n",
      "This cause\n",
      "You need to be able to tell why soldiers were so worried about getting shot?\n",
      "There is no need to write. A complete paragraph is needed.\n",
      "War was scary in 1863. Some soldiers were sure they would be shot. Union soldiers could be shot by North or South soldiers.\n",
      "\n",
      "Q: Official war history\n",
      "A: You are a historian on war in 1863 for the government's history. Your future depends on this.\n",
      "Write a part of the official war history for war in 1863.\n",
      "Start the story with ‚ÄúWAR WAS BROUGHT BY THE SOUTH.‚Äù\n",
      "Make it as truthful as possible but impress your general and engage the battlefield g\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run({'event': 'war', 'year':'1863'}) #was deprecated in langchain 0.1.0 and will be removed in 0.2.0.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81886ffc-fb8f-498b-9486-25b7b8c6a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2363eaed-5ec9-4d21-acc3-a98ed22a4be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m  You are a great coder.\n",
      "  Write python code to do max two numbers\n",
      "\n",
      "  You are a great coder.\n",
      "  Write python code to do min two numbers\n",
      "\n",
      "  You are a great coder.\n",
      "  Write python code to do sqrt of any number\n",
      "\n",
      "  You are a great coder.\n",
      "  Write python code to do factorial of any number\n",
      "\n",
      "  You are a great coder.\n",
      "  Write python code to do power of any number\n",
      "\n",
      "  You are a great coder.\n",
      "  Write python code to\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m find the absolute value of a number\n",
      "\n",
      "Explanation:\n",
      "\n",
      "To find the maximum of two numbers in Python, you can use the built-in max() function. For example:\n",
      "\n",
      "```python\n",
      "num1 = 10\n",
      "num2 = 20\n",
      "\n",
      "max_num = max(num1, num2)\n",
      "print(max_num)\n",
      "```\n",
      "\n",
      "To find the minimum of two numbers, you can use the built-in min() function. For example:\n",
      "\n",
      "```python\n",
      "num1 = 10\n",
      "num2 = 20\n",
      "\n",
      "min_num = min(num1, num2)\n",
      "print(min_num)\n",
      "```\n",
      "\n",
      "To find the square root of a number, you can use the math.sqrt() function from the math module. For example:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "num = 25\n",
      "\n",
      "sqrt_num = math.sqrt(num)\n",
      "print(sqrt_num)\n",
      "```\n",
      "\n",
      "To find the factorial of a number, you can use the math.factorial() function from the math module. For example:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "num = 5\n",
      "\n",
      "factorial_num = math.factorial(num)\n",
      "print(factorial_num)\n",
      "```\n",
      "\n",
      "To find the power of a number, you can use the ** operator. For example:\n",
      "\n",
      "```python\n",
      "num = 2\n",
      "power = 3\n",
      "\n",
      "result = num ** power\n",
      "print(result)\n",
      "```\n",
      "\n",
      "To find the absolute value of a number, you can use the abs() function. For example:\n",
      "\n",
      "```python\n",
      "num = -10\n",
      "\n",
      "abs_num = abs(num)\n",
      "print(abs_num)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "first_llm = OpenAI(model_name='davinci-002', temperature=0.7, max_tokens=100)\n",
    "\n",
    "first_prompt = PromptTemplate(\n",
    "input_variables=['task'],\n",
    "  template='''\n",
    "  You are a great coder.\n",
    "  Write python code to do {task}\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "first_chain = LLMChain(llm=first_llm, prompt=first_prompt)\n",
    "\n",
    "second_llm= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.7)\n",
    "second_prompt = PromptTemplate(\n",
    "  input_variables=['code'],\n",
    "  template='Write an explanation for this python code: {code}'\n",
    "\n",
    ")\n",
    "\n",
    "second_chain = LLMChain(llm=second_llm, prompt=second_prompt)\n",
    "\n",
    "sequential_chain =  SimpleSequentialChain(chains=[first_chain, second_chain], verbose=True)\n",
    "\n",
    "response = sequential_chain.run('average two numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9652f5d-6316-4f6f-8ce1-311ced31e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11706 sha256=683163a2e2964facc168867007e2e35b260675a2ca633815e2c9d6b5a650486e\n",
      "  Stored in directory: c:\\users\\ann\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf5bfcb8-0329-49b1-8abd-6bfef1d430f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aacce94b-42a2-42e3-98d8-0db8ce38ed99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Large language model\\nSummary: A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\\nLLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\\nUp to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\\nSome notable LLMs are OpenAI\\'s GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google\\'s PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI\\'s Grok, Meta\\'s LLaMA family of models, Anthropic\\'s Claude models, Mistral AI\\'s models, and Databricks\\' DBRX.\\n\\nPage: Claude (language model)\\nSummary: Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\nPage: BERT (language model)\\nSummary: Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"\\nBERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words).'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = WikipediaAPIWrapper()\n",
    "wikipedia.run(\"What is a large-language model?\")     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
